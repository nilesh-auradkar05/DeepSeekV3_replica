# DeepSeek-V3 Nano Base Configuration

# ============= Model Selection ==============
model:
  preset: "nano"  # Options: nano, small, 1b, medium

# ============= Data =========================
data:
  dataset_name: "HuggingFaceFW/fineweb-edu"
  train_split: "sample-10BT"
  tokenizer: "deepseek-ai/DeepSeek-v3"
  max_seq_len: 2048
  max_tokens: 5_000_000_000
  num_workers: 4

# ============= Training =========================
training:
  # Batch settings
  batch_size: 32
  gradient_accumulation_steps: 4
  max_steps: 26_000

  # Learning rate AdamW
  learning_rate: 3.0e-4
  min_learning_rate: 3.0e-5
  weight_decay: 0.1

  # Schedule
  warmup_steps: 2000
  lr_scheduler: "cosine"

  # Optimization
  optimizer: "muon"
  muon_lr: 0.02
  adam_beta1: 0.9
  adam_beta2: 0.95
  adam_epsilon: 1.0e-8
  max_grad_norm: 1.0

  # Mixed precision
  precision: "bf16"

  # MoE load balancing
  moe_bias_update_speed: 0.001

# ============ Checkpointing =====================
checkpointing:
  save_steps: 1000
  save_top_k: 2
  output_dir: "checkpoints/base"
  resume_from: null

# ============ Validation ========================
validation:
  eval_steps: 500
  eval_samples: 1000

# ============ Logging ===========================
logging:
  log_steps: 10

  # W&B logging (optional setup)
  use_wandb: false
  wandb_project: "deepseek-v3-nano"
  wandb_entity: null

  # MLflow
  use_mlflow: true
  mlflow_experiment: "deepseek-v3-nano"
  mlflow_tracking_uri: "databricks"
  mlflow_registry_uri: "databricks-uc"
  mlflow_run_name: "deepseek-v3-nano-base-v1"
  databricks_catalog: "ml_models"
  databricks_schema: "deepseek-v3"

# ============ Hardware =========================
hardware:
  num_gpus: 1
  num_workers: 4
  pin_memory: true

  # Distributed settings (for multi-GPU)
  strategy: "auto"
  distributed_backend: "nccl"

  # Memory optimization
  gradient_checkpointing: true

seed: 47
deterministic: false